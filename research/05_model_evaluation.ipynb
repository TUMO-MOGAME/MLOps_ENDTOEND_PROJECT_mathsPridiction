{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0fe6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('C:/ALL_FROM_DESKTOP/Data_Science_ENDtoEND proj/proj_1/src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0f28b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\ALL_FROM_DESKTOP\\\\Data_Science_ENDtoEND proj\\\\proj_1\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313c35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('c:\\\\ALL_FROM_DESKTOP\\\\Data_Science_ENDtoEND proj\\\\proj_1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f340a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\ALL_FROM_DESKTOP\\\\Data_Science_ENDtoEND proj\\\\proj_1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684e66e",
   "metadata": {},
   "source": [
    "# **Entity/init.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bc6335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    model_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    evaluation_report_path: Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874dc982",
   "metadata": {},
   "source": [
    "# **Config/configuration.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87dbd111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mathematicsScore.constants import *\n",
    "from src.mathematicsScore.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "240dd247",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            model_path=config.model_path,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            evaluation_report_path=config.evaluation_report_path\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00405172",
   "metadata": {},
   "source": [
    "# **Components/model_evaluation.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76106b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config file path: C:\\ALL_FROM_DESKTOP\\Data_Science_ENDtoEND proj\\proj_1\\config\\config.yaml\n",
      "Params file path: C:\\ALL_FROM_DESKTOP\\Data_Science_ENDtoEND proj\\proj_1\\params.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from src.mathematicsScore.logging import logger\n",
    "from src.mathematicsScore.config.configuration import ConfigurationManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b35e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model using test data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the trained model\n",
    "            logger.info(f\"Loading trained model from {self.config.model_path}\")\n",
    "            \n",
    "            with open(self.config.model_path, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            \n",
    "            logger.info(\"Model loaded successfully\")\n",
    "\n",
    "            # Load the transformed training data\n",
    "            logger.info(f\"Loading training data from {self.config.train_data_path}\")\n",
    "            train_data = pd.read_csv(self.config.train_data_path)\n",
    "            logger.info(f\"Training data loaded with shape: {train_data.shape}\")\n",
    "\n",
    "            # Load the transformed test data\n",
    "            logger.info(f\"Loading test data from {self.config.test_data_path}\")\n",
    "            test_data = pd.read_csv(self.config.test_data_path)\n",
    "            logger.info(f\"Test data loaded with shape: {test_data.shape}\")\n",
    "\n",
    "            # Determine target column\n",
    "            target_column = 'math_score'\n",
    "            if 'mathematics_score' in train_data.columns:\n",
    "                target_column = 'mathematics_score'\n",
    "\n",
    "            # Split features and target for training data\n",
    "            X_train = train_data.drop(columns=[target_column])\n",
    "            y_train = train_data[target_column]\n",
    "\n",
    "            # Split features and target for test data\n",
    "            X_test = test_data.drop(columns=[target_column])\n",
    "            y_test = test_data[target_column]\n",
    "\n",
    "            logger.info(f\"Evaluation features shape: {X_test.shape}\")\n",
    "            logger.info(f\"Evaluation target shape: {y_test.shape}\")\n",
    "\n",
    "            # Make predictions\n",
    "            logger.info(\"Making predictions...\")\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            # Calculate metrics\n",
    "            logger.info(\"Calculating evaluation metrics...\")\n",
    "            train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "            \n",
    "            train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "            test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            \n",
    "            train_r2 = r2_score(y_train, y_train_pred)\n",
    "            test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "            # Log metrics\n",
    "            logger.info(\"=== Model Performance Metrics ===\")\n",
    "            logger.info(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "            logger.info(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "            logger.info(f\"Training MAE: {train_mae:.4f}\")\n",
    "            logger.info(f\"Test MAE: {test_mae:.4f}\")\n",
    "            logger.info(f\"Training R²: {train_r2:.4f}\")\n",
    "            logger.info(f\"Test R²: {test_r2:.4f}\")\n",
    "\n",
    "            # Save evaluation metrics\n",
    "            metrics = {\n",
    "                'train_rmse': float(train_rmse),\n",
    "                'test_rmse': float(test_rmse),\n",
    "                'train_mae': float(train_mae),\n",
    "                'test_mae': float(test_mae),\n",
    "                'train_r2': float(train_r2),\n",
    "                'test_r2': float(test_r2),\n",
    "                'target_column': target_column,\n",
    "                'model_path': str(self.config.model_path)\n",
    "            }\n",
    "\n",
    "            # Save metrics as JSON\n",
    "            import json\n",
    "            with open(self.config.evaluation_report_path, 'w') as f:\n",
    "                json.dump(metrics, f, indent=4)\n",
    "            \n",
    "            logger.info(f\"Evaluation report saved to {self.config.evaluation_report_path}\")\n",
    "            logger.info(\"Model evaluation completed successfully\")\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model evaluation: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline_test",
   "metadata": {},
   "source": [
    "# **Pipeline Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pipeline_imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mathematicsScore.config.configuration import ConfigurationManager\n",
    "from src.mathematicsScore.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pipeline_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "train_rmse: 5.323096836206968\n",
      "test_rmse: 5.394377988346398\n",
      "train_mae: 4.266645839367169\n",
      "test_mae: 4.213968765249189\n",
      "train_r2: 0.8743150325681966\n",
      "test_r2: 0.8804162685332191\n",
      "target_column: math_score\n",
      "model_path: artifacts/model_trainer/model.pkl\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    metrics = model_evaluation.evaluate()\n",
    "    logger.info(f\"Model evaluation completed successfully\")\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty_cell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
